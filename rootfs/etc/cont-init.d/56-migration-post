#!/usr/bin/with-contenv bash
# shellcheck shell=bash
set -e

# shellcheck source=../cont-func.d/global
source /etc/cont-func.d/global

if psql_is_migration; then
  # Check if migration was succesful
  if ! psql_is_migration_success; then
    log-init "migration failed, exiting"
    exit 62
  fi

  # Update the version in PG_VERSION_FULL for future migrations
  psql_store_version

  # Generate statistics to make database usable
  if is_enabled "${PG_MIGRATE_ANALYSE}" || [ "${PG_MIGRATE_ANALYSE,,}" == "fast" ]; then
    if is_enabled "${PG_MIGRATE_ANALYSE}"; then
      # run analyse-in-stages
      log-init "generating statistics (analyse-in-stages)"
      s6-setuidgid abc exec env PGPASSWORD="${PG_PASS}" vacuumdb --all --no-password -U "${PG_USER}" --jobs="$(nproc)" --analyze-in-stages > "${PG_LOG_DIR}/analyse-v${PG_OLD_VERSION}-v${PG_VERSION_MAJOR}.log"
    fi

    if [ "${PG_MIGRATE_ANALYSE,,}" == "fast" ]; then
      # run fast analyse
      log-init "generating statistics (analyse-only)"
      s6-setuidgid abc exec env PGPASSWORD="${PG_PASS}" vacuumdb --all --no-password -U "${PG_USER}" --jobs="$(nproc)" --analyze-only > "${PG_LOG_DIR}/analyse-v${PG_OLD_VERSION}-v${PG_VERSION_MAJOR}.log"
    fi
  fi

  # Delete auto-generated scripts
  if [ -f "${PG_HOME}/analyze_new_cluster.sh" ]; then
    rm -f "${PG_HOME}/analyze_new_cluster.sh"
  fi

  if [ -f "${PG_HOME}/delete_old_cluster.sh" ]; then
    rm -f "${PG_HOME}/delete_old_cluster.sh"
  fi

  # Upgrade extensions
  if [ -f "${PG_HOME}/update_extensions.sql" ]; then
    log-init "updating extensions"
    if psql -U "${PG_USER}" -qt --file="${PG_HOME}/update_extensions.sql" > "${PG_LOG_DIR}/update-extensions-v${PG_OLD_VERSION}-v${PG_VERSION_SLUG}.log"; then
      rm -f "${PG_HOME}/update_extensions.sql"
    else
      exit 63
    fi
  fi

  # Cleanup backup if enabled
  if is_enabled "${PG_MIGRATE_DATA_BACKUP_CLEANUP}"; then
    BACKUP_DIR="${PG_HOME}/${PG_OLD_VERSION}.backup"

    log-init "removing backup directory: ${BACKUP_DIR}"
    rm -rf "${BACKUP_DIR}"
  fi

  # Cleanup old data if enabled
  if is_enabled "${PG_MIGRATE_OLD_DATA_REMOVE}"; then
    log-init "removing old data: enabled (PG_MIGRATE_OLD_DATA_REMOVE)"
    PG_OLD_DATADIR="${PG_HOME}/${PG_OLD_VERSION}"

    log-init "removing old data directory: ${PG_OLD_DATADIR}"
    rm -rf "${PG_OLD_DATADIR}"
  else
    log-init "removing old data: disabled (PG_MIGRATE_OLD_DATA_REMOVE)"
    log-init "please do not forget to remove old data" "warn"
  fi

  # exit migration mode
  log-init "migration completed; exiting migration mode"
  env_del PG_MIGRATION
  env_del PG_MIGRATION_SUCCESS
  env_del PG_OLD_VERSION
  env_del PG_OLD_VERSION_MAJOR
else
  # No migration
  #
  # However we are going to do a double check here.
  # There might be a scenario where a user wants to move their existing data
  # to this container to benefit from the functionality of this container.
  #
  # To support such a scenario we must ensure that when the user creates a
  # volume mount for the data we ensure that we will be creating the .rootfs
  # directory if required. This is to ensure a user can migrate easily from
  # on other install to this container.
  #
  # In this scenario we must also ensure that the PG_VERSION_FULL file is created.
  #
  if [[ ! -f ${PG_DATA_DIR}/PG_VERSION_FULL ]]; then
    psql_store_version
  fi

  psql_create_old_rootfs
fi
